{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"wk_LICT_session10_action_recognition.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"S3tDSz7s3ZCe","colab_type":"text"},"source":["# LICT-NUS AI Program\n","# Session 10 workshop on human action recognition\n","\n","Contact: Dr. Tian Jing\n","\n","Email: tianjing@nus.edu.sg\n","\n","# Objective\n","In this workshop, we will perform the following three tasks\n","\n","- Exercise 1: Perform action recognition using histogram of optical flow\n","- Exercise 2: Perform action recognition using C3D deep learning approach\n","\n","\n","# Installation guideline (Colab)\n","[Last test on 01 June 2020] All required libraries have been installed in Google Colab.\n","\n","# Installation guideline (local machine)\n","- Open Anaconda Prompt\n","\n","- Append the channel `conda-forge` into your conda configuration.\n","\n","`conda config --append channels conda-forge`\n","\n","- Create a new virtual environment `rtavs` or install additional packages in your own environment\n","\n","**[Windows, CPU version]**\n","\n","`conda create -n rtavs python=3.6 numpy=1.15.1 opencv=3.4.2 matplotlib=2.2.3 tensorflow=1.12.0 scipy=1.1.0 scikit-learn=0.19.1 spyder=3.3.2 yaml=0.1.7 keras=2.2.4 pillow=5.4.1 notebook=5.7.4 pandas=0.24.2 h5py=2.8.0`\n","\n","**[Windows, GPU version, CUDA 9.0]**\n","\n","`conda create -n rtavs python=3.6 numpy=1.15.1 opencv=3.4.2 matplotlib=2.2.3 tensorflow-gpu=1.12.0 scipy=1.1.0 scikit-learn=0.19.1 spyder=3.3.2 yaml=0.1.7 keras-gpu=2.2.4 pillow=5.4.1 notebook=5.7.4 pandas=0.24.2 h5py=2.8.0`\n","\n","- Activate the environment `rtavs`\n","\n","`conda activate rtavs`\n","\n","- Browse to the folder that contains the workshop files, then run Jupyter Notebook\n","\n","`jupyter notebook`\n","\n","# Submission guideline\n","\n","Once you finish the workshop, rename your .ipynb file to be your name, and submit your .ipynb file into LumiNUS."]},{"cell_type":"code","metadata":{"id":"Ka5Ks_0U3ylR","colab_type":"code","outputId":"d4653cd7-3533-442b-a53e-c7fa47a0f76d","executionInfo":{"status":"ok","timestamp":1591359160111,"user_tz":-480,"elapsed":7998,"user":{"displayName":"Jing Tian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3IrdHRRL-VTM2y_vcHIL4BN6hovl7bSbbIjU=s64","userId":"12881665472836024238"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"source":["# Check GPU setup in Colab\n","%tensorflow_version 1.x\n","import tensorflow as tf\n","\n","# Silence the tensorflow warning message\n","tf.get_logger().setLevel('ERROR')\n","\n","# Check GPU coinfiguration in Colab (T4 GPU)\n","print(\"Tensorflow version: \", tf.__version__)\n","print(tf.test.gpu_device_name())\n","# Your expected output will be '/device:GPU:0'"],"execution_count":1,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n","Tensorflow version:  1.15.2\n","/device:GPU:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IkzWnZdu3zP_","colab_type":"code","outputId":"831f49b1-801f-458b-e2ae-2d8aad32e11f","executionInfo":{"status":"ok","timestamp":1591359184268,"user_tz":-480,"elapsed":32146,"user":{"displayName":"Jing Tian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3IrdHRRL-VTM2y_vcHIL4BN6hovl7bSbbIjU=s64","userId":"12881665472836024238"}},"colab":{"base_uri":"https://localhost:8080/","height":183}},"source":["# Mount your drive\n","# Run this cell, then you’ll see a link, click on that link, allow access\n","# Copy the code that pops up, Paste it in the box, Hit enter\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","# Change working directory to be current folder\n","import os\n","os.chdir('/content/gdrive/My Drive/RTAVS/action')\n","!ls"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n","data\t\t\t    model_c3d_v0815.h5\n","data_test_hof_feature.npz   wk_action_colab_v3.0.ipynb\n","data_train_hof_feature.npz  wk_LICT_session10_action_recognition.ipynb\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3eZH1wWw3ZCf","colab_type":"code","outputId":"68a1cca0-b442-48e0-dd78-dc92b62f7f5b","executionInfo":{"status":"ok","timestamp":1591359185820,"user_tz":-480,"elapsed":33694,"user":{"displayName":"Jing Tian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3IrdHRRL-VTM2y_vcHIL4BN6hovl7bSbbIjU=s64","userId":"12881665472836024238"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["import cv2\n","import os\n","import numpy as np\n","import random\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","from scipy import sqrt, pi, arctan2, cos, sin # used for HoF\n","from scipy.ndimage import uniform_filter # used for hoF\n","from sklearn import svm\n","\n","from keras.layers import Activation, Conv3D, Dense, Dropout, Flatten, MaxPooling3D\n","from keras.losses import categorical_crossentropy\n","from keras.models import Sequential, Model\n","from keras.models import load_model\n","from keras.optimizers import Adam\n","from keras.utils import np_utils\n","from sklearn.metrics import confusion_matrix\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"DQI-9bZt3ZCj","colab_type":"text"},"source":["# Explore the dataset\n","\n","- UCF11 Dataset: https://www.crcv.ucf.edu/data/UCF_YouTube_Action.php.\n","\n","It contains 11 action categories: basketball shooting, biking/cycling, diving, golf swinging, horse back riding, soccer juggling, swinging, tennis swinging, trampoline jumping, volleyball spiking, and walking with a dog.\n"]},{"cell_type":"code","metadata":{"id":"Xgx-bDIe3ZCj","colab_type":"code","colab":{}},"source":["def load_groups(input_folder):\n","    '''\n","    Load the list of sub-folders into a python list with their\n","    corresponding label.\n","    '''\n","    groups         = []\n","    label_folders  = os.listdir(input_folder)\n","    index          = 0\n","    for label_folder in sorted(label_folders):\n","        label_folder_path = os.path.join(input_folder, label_folder)\n","        if os.path.isdir(label_folder_path):\n","            group_folders = os.listdir(label_folder_path)\n","            for group_folder in group_folders:\n","                if group_folder != 'Annotation':\n","                    groups.append([os.path.join(label_folder_path, group_folder), index])\n","            index += 1\n","\n","    return groups\n","\n","#Reference: https://github.com/microsoft/CNTK/blob/master/Examples/Video/DataSets/UCF11/split_ucf11.py\n","def ucf_split_data(groups, file_ext):\n","    '''\n","    Split the data at random for train, eval and test set.\n","    '''\n","    group_count = len(groups)\n","    indices = np.arange(group_count)\n","\n","    np.random.seed(0) # Make it deterministic.\n","    np.random.shuffle(indices)\n","\n","    # 80% training and 20% test.\n","    train_count = int(0.8 * group_count)\n","    test_count  = group_count - train_count\n","\n","    train = []\n","    test  = []\n","\n","    for i in range(train_count):\n","        group = groups[indices[i]]\n","        video_files = os.listdir(group[0])\n","        for video_file in video_files:\n","            video_file_path = os.path.join(group[0], video_file)\n","            if os.path.isfile(video_file_path):\n","                video_file_path = os.path.abspath(video_file_path)\n","                ext = os.path.splitext(video_file_path)[1]\n","                if (ext == file_ext):\n","                    train.append([video_file_path, group[1]])\n","\n","    for i in range(train_count, train_count + test_count):\n","        group = groups[indices[i]]\n","        video_files = os.listdir(group[0])\n","        for video_file in video_files:\n","            video_file_path = os.path.join(group[0], video_file)\n","            if os.path.isfile(video_file_path):\n","                video_file_path = os.path.abspath(video_file_path)\n","                ext = os.path.splitext(video_file_path)[1]\n","                if (ext == file_ext):\n","                    test.append([video_file_path, group[1]])\n","\n","    return train, test\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XM6_ZF3B3ZCm","colab_type":"code","outputId":"3023a73f-a856-4f5c-8f89-7408a568723e","executionInfo":{"status":"ok","timestamp":1591359205063,"user_tz":-480,"elapsed":52930,"user":{"displayName":"Jing Tian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3IrdHRRL-VTM2y_vcHIL4BN6hovl7bSbbIjU=s64","userId":"12881665472836024238"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["# Prepare the dataset\n","ucf_groups = load_groups(\"data\")\n","\n","ucf_action_labels  = os.listdir(\"data\")\n","print(\"action labels: \", ucf_action_labels)\n","\n","ucf_train, ucf_test = ucf_split_data(ucf_groups, '.avi')\n","print(\"Total %d categories, Training data %d sequences, test data %d sequences\" % (len(ucf_action_labels), len(ucf_train), len(ucf_test)))\n","\n"],"execution_count":5,"outputs":[{"output_type":"stream","text":["action labels:  ['basketball', 'biking', 'diving', 'tennis_swing', 'volleyball_spiking', 'golf_swing', 'horse_riding', 'soccer_juggling', 'trampoline_jumping', 'swing', 'walking']\n","Total 11 categories, Training data 1295 sequences, test data 305 sequences\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"q-E54zWM3ZCp","colab_type":"text"},"source":["# Exercise 1: Action recognition using histogram of optical flow\n","\n","- Reference: Histogram of optical flow,  https://github.com/colincsl/pyKinectTools/blob/master/pyKinectTools/algs/HistogramOfOpticalFlow.py"]},{"cell_type":"code","metadata":{"id":"dkEcVRtI3ZCp","colab_type":"code","colab":{}},"source":["# Reference: https://github.com/colincsl/pyKinectTools/blob/master/pyKinectTools/algs/HistogramOfOpticalFlow.py\n","# Fix a few bugs\n","def hof(flow, orientations=9, pixels_per_cell=(8, 8),\n","        cells_per_block=(2, 2), normalise=False, motion_threshold=1.):\n","\n","    \"\"\"Extract Histogram of Optical Flow (HOF) for a given image.\n","    Key difference between this and HOG is that flow is MxNx2 instead of MxN\n","    Compute a Histogram of Optical Flow (HOF) by\n","        1. (optional) global image normalisation\n","        2. computing the dense optical flow\n","        3. computing flow histograms\n","        4. normalising across blocks\n","        5. flattening into a feature vector\n","    Parameters\n","    ----------\n","    Flow : (M, N) ndarray\n","        Input image (x and y flow images).\n","    orientations : int\n","        Number of orientation bins.\n","    pixels_per_cell : 2 tuple (int, int)\n","        Size (in pixels) of a cell.\n","    cells_per_block  : 2 tuple (int,int)\n","        Number of cells in each block.\n","    normalise : bool, optional\n","        Apply power law compression to normalise the image before\n","        processing.\n","    static_threshold : threshold for no motion\n","    Returns\n","    -------\n","    newarr : ndarray\n","        hof for the image as a 1D (flattened) array.\n","    hof_image : ndarray (if visualise=True)\n","        A visualisation of the hof image.\n","    References\n","    ----------\n","    * http://en.wikipedia.org/wiki/Histogram_of_oriented_gradients\n","    * Dalal, N and Triggs, B, Histograms of Oriented Gradients for\n","      Human Detection, IEEE Computer Society Conference on Computer\n","      Vision and Pattern Recognition 2005 San Diego, CA, USA\n","    \"\"\"\n","    flow = np.atleast_2d(flow)\n","\n","    \"\"\" \n","    -1-\n","    The first stage applies an optional global image normalisation\n","    equalisation that is designed to reduce the influence of illumination\n","    effects. In practice we use gamma (power law) compression, either\n","    computing the square root or the log of each colour channel.\n","    Image texture strength is typically proportional to the local surface\n","    illumination so this compression helps to reduce the effects of local\n","    shadowing and illumination variations.\n","    \"\"\"\n","\n","    if flow.ndim < 3:\n","        raise ValueError(\"Requires dense flow in both directions\")\n","\n","    if normalise:\n","        flow = sqrt(flow)\n","\n","    \"\"\" \n","    -2-\n","    The second stage computes first order image gradients. These capture\n","    contour, silhouette and some texture information, while providing\n","    further resistance to illumination variations. The locally dominant\n","    colour channel is used, which provides colour invariance to a large\n","    extent. Variant methods may also include second order image derivatives,\n","    which act as primitive bar detectors - a useful feature for capturing,\n","    e.g. bar like structures in bicycles and limbs in humans.\n","    \"\"\"\n","\n","    if flow.dtype.kind == 'u':\n","        # convert uint image to float\n","        # to avoid problems with subtracting unsigned numbers in np.diff()\n","        flow = flow.astype('float')\n","\n","    gx = np.zeros(flow.shape[:2])\n","    gy = np.zeros(flow.shape[:2])\n","    # gx[:, :-1] = np.diff(flow[:,:,1], n=1, axis=1)\n","    # gy[:-1, :] = np.diff(flow[:,:,0], n=1, axis=0)\n","\n","    gx = flow[:,:,1]\n","    gy = flow[:,:,0]\n","\n","\n","    \"\"\" \n","    -3-\n","    The third stage aims to produce an encoding that is sensitive to\n","    local image content while remaining resistant to small changes in\n","    pose or appearance. The adopted method pools gradient orientation\n","    information locally in the same way as the SIFT [Lowe 2004]\n","    feature. The image window is divided into small spatial regions,\n","    called \"cells\". For each cell we accumulate a local 1-D histogram\n","    of gradient or edge orientations over all the pixels in the\n","    cell. This combined cell-level 1-D histogram forms the basic\n","    \"orientation histogram\" representation. Each orientation histogram\n","    divides the gradient angle range into a fixed number of\n","    predetermined bins. The gradient magnitudes of the pixels in the\n","    cell are used to vote into the orientation histogram.\n","    \"\"\"\n","\n","    magnitude = sqrt(gx**2 + gy**2)\n","    orientation = arctan2(gy, gx) * (180 / pi) % 180\n","\n","    sy, sx = flow.shape[:2]\n","    cx, cy = pixels_per_cell\n","    bx, by = cells_per_block\n","\n","    n_cellsx = int(np.floor(sx // cx))  # number of cells in x\n","    n_cellsy = int(np.floor(sy // cy))  # number of cells in y\n","\n","    # compute orientations integral images\n","    orientation_histogram = np.zeros((n_cellsy, n_cellsx, orientations))\n","    subsample = np.index_exp[int(cy / 2):cy * n_cellsy:cy, int(cx / 2):cx * n_cellsx:cx]\n","    for i in range(orientations-1):\n","        #create new integral image for this orientation\n","        # isolate orientations in this range\n","\n","        temp_ori = np.where(orientation < 180 / orientations * (i + 1),\n","                            orientation, -1)\n","        temp_ori = np.where(orientation >= 180 / orientations * i,\n","                            temp_ori, -1)\n","        # select magnitudes for those orientations\n","        cond2 = (temp_ori > -1) * (magnitude > motion_threshold)\n","        temp_mag = np.where(cond2, magnitude, 0)\n","\n","        temp_filt = uniform_filter(temp_mag, size=(cy, cx))\n","        orientation_histogram[:, :, i] = temp_filt[subsample]\n","\n","    ''' Calculate the no-motion bin '''\n","    temp_mag = np.where(magnitude <= motion_threshold, magnitude, 0)\n","\n","    temp_filt = uniform_filter(temp_mag, size=(cy, cx))\n","    orientation_histogram[:, :, -1] = temp_filt[subsample]\n","\n","    \"\"\"\n","    The fourth stage computes normalisation, which takes local groups of\n","    cells and contrast normalises their overall responses before passing\n","    to next stage. Normalisation introduces better invariance to illumination,\n","    shadowing, and edge contrast. It is performed by accumulating a measure\n","    of local histogram \"energy\" over local groups of cells that we call\n","    \"blocks\". The result is used to normalise each cell in the block.\n","    Typically each individual cell is shared between several blocks, but\n","    its normalisations are block dependent and thus different. The cell\n","    thus appears several times in the final output vector with different\n","    normalisations. This may seem redundant but it improves the performance.\n","    We refer to the normalised block descriptors as Histogram of Oriented\n","    Gradient (hog) descriptors.\n","    \"\"\"\n","\n","    n_blocksx = (n_cellsx - bx) + 1\n","    n_blocksy = (n_cellsy - by) + 1\n","    normalised_blocks = np.zeros((n_blocksy, n_blocksx,\n","                                  by, bx, orientations))\n","\n","    for x in range(n_blocksx):\n","        for y in range(n_blocksy):\n","            block = orientation_histogram[y:y+by, x:x+bx, :]\n","            eps = 1e-5\n","            normalised_blocks[y, x, :] = block / sqrt(block.sum()**2 + eps)\n","\n","    return normalised_blocks.ravel()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"a4TF7vqH3ZCs","colab_type":"code","colab":{}},"source":["# Define the HoF feature extraction function\n","def extract_hof_feature(video_list):\n","    feature_hof = []\n","    label_list = []\n","    img_width = 128\n","    img_height = 64\n","    for idx, value in enumerate(video_list):\n","        # Display the progress\n","        if (idx % 100) == 0:\n","            print(\"process sequence %d/%d\" % (idx, len(video_list)))\n","        filename = value[0]\n","        label = value[1]\n","        hof_feature_all = []\n","\n","        cap = cv2.VideoCapture(filename)\n","        ret, frame = cap.read()\n","        if ret:\n","            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","            gray = cv2.resize(gray, (img_width, img_height)) # Resize frames to reduce feature dimensions\n","        \n","            while True:\n","                previousGray = gray\n","                ret, frame = cap.read()\n","\n","                if ret:\n","                    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","                    gray = cv2.resize(gray, (img_width, img_height))\n","                    flow = cv2.calcOpticalFlowFarneback(previousGray, gray, flow=None, pyr_scale=0.5, levels=5, winsize=11, iterations=10, poly_n=5, poly_sigma=1.1, flags=0)\n","                    hof_feature_one = hof(flow, orientations=9, pixels_per_cell=(8, 8),cells_per_block=(2, 2))\n","                    if (len(hof_feature_all) == 0):\n","                        hof_feature_all = hof_feature_one\n","                    else:\n","                        hof_feature_all = np.vstack((hof_feature_all, hof_feature_one))\n","                else:\n","                    break\n","    \n","        cap.release()\n","        if (len(hof_feature_all) != 0):\n","            hof_feature_mean = np.mean(hof_feature_all, axis=0)\n","            feature_hof.append(hof_feature_mean)\n","            label_list.append(label)      \n","        \n","    return np.array(feature_hof), np.array(label_list)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CC0_4bBx3ZCu","colab_type":"code","colab":{}},"source":["# It takes around half hour to prepare such feature dataset\n","# You can uncomment if you want to re-build feature dataset, otherwise, load them from the data files\n","# The sequence file basketball\\v_shooting_24\\v_shooting_24_01.avi is very short.\n","\n","# print(\"Prepare training feature dataset\")\n","# train_feature_hof, train_label_hof = extract_hof_feature(ucf_train)\n","# np.savez(\"data_train_hof_feature.npz\", X=train_feature_hof, Y=train_label_hof)\n","# print(train_feature_hof.shape, train_label_hof.shape)\n","\n","# print(\"Prepare test feature dataset\")\n","# test_feature_hof, test_label_hof = extract_hof_feature(ucf_test)\n","# np.savez(\"data_test_hof_feature.npz\", X=test_feature_hof, Y=test_label_hof)\n","# print(test_feature_hof.shape, test_label_hof.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"R5POsmAh3ZCx","colab_type":"code","outputId":"01f60128-d340-4619-c967-68a6a3a47c20","executionInfo":{"status":"ok","timestamp":1591359206455,"user_tz":-480,"elapsed":54312,"user":{"displayName":"Jing Tian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3IrdHRRL-VTM2y_vcHIL4BN6hovl7bSbbIjU=s64","userId":"12881665472836024238"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["# Load HoF features from pre-prepared data file and perform SVM classification\n","with np.load(\"data_train_hof_feature.npz\") as npzfile:\n","    x_train_hof = npzfile[\"X\"]\n","    x_train_hof_label = npzfile[\"Y\"]\n","    \n","with np.load(\"data_test_hof_feature.npz\") as npzfile:\n","    x_test_hof = npzfile[\"X\"]\n","    x_test_hof_label = npzfile[\"Y\"]\n","    \n","print(\"Training data\", x_train_hof.shape, x_train_hof_label.shape)\n","print(\"Test data\", x_test_hof.shape, x_test_hof_label.shape)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Training data (1293, 3780) (1293,)\n","Test data (306, 3780) (306,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-B_wJ3xB3ZC0","colab_type":"code","outputId":"db2d230b-f658-4414-c4ac-578ee295983a","executionInfo":{"status":"ok","timestamp":1591359212809,"user_tz":-480,"elapsed":60661,"user":{"displayName":"Jing Tian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3IrdHRRL-VTM2y_vcHIL4BN6hovl7bSbbIjU=s64","userId":"12881665472836024238"}},"colab":{"base_uri":"https://localhost:8080/","height":217}},"source":["hof_svm_model = svm.SVC(kernel = 'linear', C = 10).fit(x_train_hof, x_train_hof_label)\n","\n","x_test_hof_pred = hof_svm_model.predict(x_test_hof)\n","\n","print(confusion_matrix(x_test_hof_label, x_test_hof_pred))\n"],"execution_count":10,"outputs":[{"output_type":"stream","text":["[[ 0  1  2  0  0  0  0  1  0  0  1]\n"," [ 4 34  0  0  4  1  1  0  0  0  1]\n"," [ 3  0  4  0  0  0  3  0  0  1  2]\n"," [ 0  0  2 24  1  1  3  1  0  3  1]\n"," [ 0  0  0  0 17  0  0  0  0  0  2]\n"," [ 6  0  0 10  0 12  0  2  1  1  0]\n"," [ 0  6  1  0  0  1 14  0  5  0  1]\n"," [ 5  0  1  5  4  5  0 24  3  3  2]\n"," [ 0  2  0  6  0  1  1  0 14  0  0]\n"," [ 3  0  5  1  2  0  0  1  0  4  2]\n"," [ 2  4  8  2  1  1  3  0  1  2 10]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yEdUiVlb3ZC2","colab_type":"text"},"source":["# Exercise 2: Action recognition using C3D model\n","\n","A modified C3D model is used in the workshop to reduce model training time for demonstration purpose.\n","\n","- Reference: D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri, “Learning Spatiotemporal Features with 3D Convolutional Networks“, ICCV 2015, https://arxiv.org/abs/1412.0767"]},{"cell_type":"code","metadata":{"id":"wijRprjc3ZC3","colab_type":"code","colab":{}},"source":["class Videoto3D:\n","\n","    def __init__(self, width, height, depth):\n","        self.width = width\n","        self.height = height\n","        self.depth = depth\n","\n","    def get_data(self, filename, skip=True):\n","        cap = cv2.VideoCapture(filename)\n","        nframe = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n","        bAppend = False\n","        if (nframe>=self.depth):\n","            if skip:\n","                frames = [x * nframe / self.depth for x in range(self.depth)]\n","            else:\n","                frames = [x for x in range(self.depth)]\n","        else:\n","            print(\"Insufficient %d frames in video %s, set bAppend as True\" % (nframe, filename))\n","            bAppend = True\n","            frames = [x for x in range(int(nframe))] # nframe is a float\n","\n","        framearray = []\n","\n","        for i in range(len(frames)):#self.depth):\n","            cap.set(cv2.CAP_PROP_POS_FRAMES, frames[i])\n","            ret, frame = cap.read()\n","            frame = cv2.resize(frame, (self.height, self.width))\n","            framearray.append(cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY))\n","\n","        cap.release()\n","        \n","        if bAppend:\n","            while len(framearray) < self.depth:\n","                framearray.append(cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY))\n","            print(\"Append more frames in the framearray to have %d frames\" % len(framearray))\n","                \n","        return np.array(framearray)\n","\n","def loaddata(video_list, vid3d, skip=True):\n","    X = []\n","    Y = []\n","    for idx, value in enumerate(video_list):\n","        # Display the progress\n","        if (idx % 100) == 0:\n","            print(\"process data %d/%d\" % (idx, len(video_list)))\n","        filename = value[0]\n","        label = value[1]\n","        Y.append(label)\n","        X.append(vid3d.get_data(filename, skip=skip))\n","        \n","    return np.array(X).transpose((0, 2, 3, 1)), np.array(Y)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NQQ8eVRf3ZC5","colab_type":"code","colab":{}},"source":["# Define parameter setting\n","class Args:\n","    batch = 128\n","    epoch = 50\n","    nclass = 11 # 11 action categories\n","    depth = 10\n","    rows = 32\n","    cols = 32\n","    skip = True # Skip: randomly extract frames; otherwise, extract first few frames\n","\n","param_setting = Args()\n","img_rows = param_setting.rows\n","img_cols = param_setting.cols\n","frames = param_setting.depth\n","channel = 1\n","vid3d = Videoto3D(img_rows, img_cols, frames)\n","nb_classes = param_setting.nclass\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FD3kE9Rr3ZC8","colab_type":"code","outputId":"2835d54f-16e3-42bd-baa7-9d1d901268db","executionInfo":{"status":"ok","timestamp":1591359765263,"user_tz":-480,"elapsed":613107,"user":{"displayName":"Jing Tian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3IrdHRRL-VTM2y_vcHIL4BN6hovl7bSbbIjU=s64","userId":"12881665472836024238"}},"colab":{"base_uri":"https://localhost:8080/","height":363}},"source":["# Prepare training data\n","x_train, y_train = loaddata(ucf_train, vid3d, param_setting.skip)\n","x_train = x_train.reshape((x_train.shape[0], img_rows, img_cols, frames, channel))\n","y_train = np_utils.to_categorical(y_train, nb_classes)  \n","\n","# Prepare test data\n","x_test, y_test = loaddata(ucf_test, vid3d, param_setting.skip)\n","x_test = x_test.reshape((x_test.shape[0], img_rows, img_cols, frames, channel))\n","y_test = np_utils.to_categorical(y_test, nb_classes)\n"],"execution_count":13,"outputs":[{"output_type":"stream","text":["process data 0/1295\n","process data 100/1295\n","process data 200/1295\n","process data 300/1295\n","process data 400/1295\n","process data 500/1295\n","process data 600/1295\n","process data 700/1295\n","process data 800/1295\n","process data 900/1295\n","process data 1000/1295\n","Insufficient 1 frames in video /content/gdrive/My Drive/RTAVS/action/data/basketball/v_shooting_24/v_shooting_24_01.avi, set bAppend as True\n","Append more frames in the framearray to have 10 frames\n","process data 1100/1295\n","process data 1200/1295\n","process data 0/305\n","process data 100/305\n","process data 200/305\n","process data 300/305\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zcMdsP5w3ZC_","colab_type":"code","outputId":"8c3b7548-89ec-4bf3-f19c-9ff826f04b82","executionInfo":{"status":"ok","timestamp":1591359765802,"user_tz":-480,"elapsed":613641,"user":{"displayName":"Jing Tian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3IrdHRRL-VTM2y_vcHIL4BN6hovl7bSbbIjU=s64","userId":"12881665472836024238"}},"colab":{"base_uri":"https://localhost:8080/","height":745}},"source":["# Define deep learning model\n","\n","c3d_model = Sequential()\n","c3d_model.add(Conv3D(32, kernel_size=(3, 3, 3), input_shape=(x_train.shape[1:]), padding='same'))\n","c3d_model.add(Activation('relu'))\n","c3d_model.add(Conv3D(32, kernel_size=(3, 3, 3), padding='same'))\n","c3d_model.add(Activation('softmax'))\n","c3d_model.add(MaxPooling3D(pool_size=(3, 3, 3), padding='same'))\n","c3d_model.add(Dropout(0.25))\n","\n","c3d_model.add(Conv3D(64, kernel_size=(3, 3, 3), padding='same'))\n","c3d_model.add(Activation('relu'))\n","c3d_model.add(Conv3D(64, kernel_size=(3, 3, 3), padding='same'))\n","c3d_model.add(Activation('softmax'))\n","c3d_model.add(MaxPooling3D(pool_size=(3, 3, 3), padding='same'))\n","c3d_model.add(Dropout(0.25))\n","\n","c3d_model.add(Flatten(name='flatten_feature'))\n","c3d_model.add(Dense(512, activation='sigmoid'))\n","c3d_model.add(Dropout(0.2))\n","c3d_model.add(Dense(nb_classes, activation='softmax'))\n","\n","c3d_model.compile(loss=categorical_crossentropy, optimizer=Adam(), metrics=['accuracy'])\n","c3d_model.summary()"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv3d_1 (Conv3D)            (None, 32, 32, 10, 32)    896       \n","_________________________________________________________________\n","activation_1 (Activation)    (None, 32, 32, 10, 32)    0         \n","_________________________________________________________________\n","conv3d_2 (Conv3D)            (None, 32, 32, 10, 32)    27680     \n","_________________________________________________________________\n","activation_2 (Activation)    (None, 32, 32, 10, 32)    0         \n","_________________________________________________________________\n","max_pooling3d_1 (MaxPooling3 (None, 11, 11, 4, 32)     0         \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 11, 11, 4, 32)     0         \n","_________________________________________________________________\n","conv3d_3 (Conv3D)            (None, 11, 11, 4, 64)     55360     \n","_________________________________________________________________\n","activation_3 (Activation)    (None, 11, 11, 4, 64)     0         \n","_________________________________________________________________\n","conv3d_4 (Conv3D)            (None, 11, 11, 4, 64)     110656    \n","_________________________________________________________________\n","activation_4 (Activation)    (None, 11, 11, 4, 64)     0         \n","_________________________________________________________________\n","max_pooling3d_2 (MaxPooling3 (None, 4, 4, 2, 64)       0         \n","_________________________________________________________________\n","dropout_2 (Dropout)          (None, 4, 4, 2, 64)       0         \n","_________________________________________________________________\n","flatten_feature (Flatten)    (None, 2048)              0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 512)               1049088   \n","_________________________________________________________________\n","dropout_3 (Dropout)          (None, 512)               0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 11)                5643      \n","=================================================================\n","Total params: 1,249,323\n","Trainable params: 1,249,323\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DpzMTZVk3ZDB","colab_type":"code","colab":{}},"source":["# # Train the model\n","# c3d_model.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=param_setting.batch,\n","#           epochs=param_setting.epoch, verbose=2, shuffle=True)\n","\n","# c3d_model.save_weights(\"data/model_c3d_v0815.h5\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZfHPV5t93ZDD","colab_type":"code","outputId":"bd692193-30e0-47cf-cccc-701a64990f35","executionInfo":{"status":"ok","timestamp":1591359779272,"user_tz":-480,"elapsed":627105,"user":{"displayName":"Jing Tian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3IrdHRRL-VTM2y_vcHIL4BN6hovl7bSbbIjU=s64","userId":"12881665472836024238"}},"colab":{"base_uri":"https://localhost:8080/","height":235}},"source":["# Load the pre-trained model\n","c3d_model.load_weights(\"model_c3d_v0815.h5\")\n","\n","# Evaluate the deep learning model\n","y_pred = c3d_model.predict(x_test, verbose=0)\n","print(\"Confusion matrix\")\n","print(confusion_matrix(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1)))\n"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Confusion matrix\n","[[ 0  0  2  0  0  0  0  0  0  3  0]\n"," [ 4 24  3  0  2  0  7  0  2  0  1]\n"," [ 0  0 13  0  0  0  0  0  0  0  1]\n"," [ 0  0  0 30  0  1  0  2  1  0  0]\n"," [ 0  4  0  0 15  0  0  0  0  0  1]\n"," [ 8  0  0  0  0 23  0  0  1  0  1]\n"," [ 0  4  1  0  0  1 21  0  1  0  1]\n"," [ 0  2  1  7  0  3  0 38  0  0  1]\n"," [ 0  0  0  0  0  0  4  0 19  0  0]\n"," [ 3  1  3  0  1  1  2  0  1  6  0]\n"," [ 0  2  2  1  1  3  1  0  2  0 22]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DWBBhV0w3ZDG","colab_type":"text"},"source":["$\\color{red}{\\text{Q1: How to use what we have developed from this workshop in your AI product?}}$\n"]},{"cell_type":"code","metadata":{"id":"4W943NjK3ZDG","colab_type":"code","colab":{}},"source":["# Provide your proposed ideas to Q1 here. (no need programming)\n","#\n","# "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"43UZE2MD3ZDI","colab_type":"text"},"source":["**Once you finish the workshop, rename your .ipynb file to be your name, and submit your .ipynb file into LumiNUS.**\n","\n","Have a nice day!"]}]}